{
  "hash": "70a77b995d3573afba9361a1936b1c3b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hands-On Introduction to Sentiment Analysis with R\nsubtitle: \"Theme: AI and Society\"\nauthor:\n    - name: KT Wong\nemail: \"kwanto@hku.hk\"\naffiliation: \"Faculty of Social Sciences, HKU\"\n\n#abstract: |\n #     The materials in this topic are drawn from @Chetty2020\n      \n \ndate: today\n\n#bibliography: reference_bigdata.bib\n\nformat:\n  revealjs:\n    theme: clean_42_rb.scss\n    embed-resources: true\n    chalkboard: false\n    multiplex: false\n    slide-number: c/t\n    preview-links: auto\n    margin: 0.03\n    height: 1080\n    width: 1920\n    max-scale: 2\n    smaller: false\n    scrollable: true\n    incremental: TRUE\n    template-partials:\n       - title-slide.html\n    controls: true\n    touch: false\n    code-fold: true\n    code-overflow: scroll\n    code-line-numbers: true\n    fig-asp: 0.618\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    \ninclude-in-header:\n  text: |\n      <style>\n      .reveal pre, .reveal pre code, .reveal .sourceCode {\n        font-size: 24px !important;\n      }\n      </style>\n      \n# execute:\n#   cache: TRUE\n#   dev: \"cairo_png\"  # Use cairo_png device\n#   python: \"C:/Users/ktwong/AppData/Local/Programs/Python/Python312/python.exe\"\n   \n\nengine: knitr\n#engine: jupyter\n#kernal: python3\n\nknitr:\n  opts_chunk:\n    echo: false\n    \nfig-cap-location: bottom\nfig-width: 12\n#fig-height: 5\n#fig-dpi: 300\nlightbox: True\n---\n\n# Main theme: **AI and Society** {visibility=\"uncounted\"}\n\n**Hands-On:** Introduction to Sentiment Analysis with R\n\n**Audience:** High school students new to R  \n\n**Duration:** 2 hours  \n\n**Environment:** R and RStudio  \n\n## Overview\n\n+ This 2-hour workshop introduces high school students to **sentiment analysis** using R in RStudio\n\n+ we will analyze social media comments about AI’s societal impact\n  + learning basic R commands\n  + exploring how sentiment analysis reveals public opinions\n  \n+ Roadmap\n  + background on sentiment analysis\n  + hands-on tasks\n  + discussions linking AI to society\n\n+ **Learning Goals:**\n  + Understand sentiment analysis and how AI can enhance it\n  + Learn basic R commands for text analysis\n  + Analyze sentiments in comments about AI’s societal impact\n  + Discuss how sentiment analysis informs AI’s role in society\n\n::: notes\n\n**Prerequisites:** No R experience needed. Students need R and RStudio installed.\n\n**Materials:** Laptops with R and RStudio, a simple dataset (provided in code).\n\n:::\n\n## Background: Sentiment Analysis and AI in Society\n\n+ What is Sentiment Analysis?\n  + Sentiment analysis is an AI technique that identifies emotions in text\n    + commonly label them\n      + positive \n        + e.g. I love this! \n      + negative \n        + e.g. This is scary\n      + neutral\n        + e.g. It’s fine\n        \n+ it is used to understand public opinions/public attitudes\n  + e.g. how people feel about AI in education, jobs, or healthcare\n\n## Background: Sentiment Analysis and AI in Society\n\n+ **How it Works:**  \n  + **Lexicon-based**\n    + Uses a dictionary to assign scores to words\n      + e.g., awesome = +3, worried = -2\n    + We mainly focus on this approach today for simplicity\n\n  + **Machine Learning-based**\n    + Trains machine learning models on labeled data to predict sentiment\n      + e.g., \"I love AI!\" → positive, \"AI is scary\" → negative\n    + More complex but powerful\n  \n  + **Advanced AI**\n    + LLM models (like ChatGPT, Deepseek) \n    + it analyze context for higher accuracy, but they are complex \n\n## Background: Sentiment Analysis and AI in Society\n\n+ **Connection to AI and Society:**  \n  + Sentiment analysis reveals public attitudes toward AI and products, helping understand its societal impact.  \n  + Examples:  \n    + Companies analyze tweets to improve their products \n    + Governments study comments to address concerns about the satifaction of its public services  \n    + Researchers explore how AI in healthcare is perceived\n      + e.g. trust in AI diagnostics  \n\n+ By analyzing text, we learn what excites or worries people, driving further development to benefit society\n\n::: notes\n\n+ AI is transforming society—powering education tools, automating jobs, and aiding doctors\n\n+ Understanding public sentiment ensures AI is developed responsibly, addressing fears (e.g., privacy) and amplifying benefits\n  + e.g., better learning\n\n:::\n\n# Hands-on Workshop {visibility=\"uncounted\"}\n\n## Step 1: Introduction and Setup\n\n+ **Objective: set up RStudio**\n\n+ **Task 1.1: Open RStudio**  \n  + Open RStudio  \n  + Create a new R script: `File > New File > R Script`\n    + Save as `sentiment_workshop.R` if needed\n\n+ **Task 1.2: Install Packages**  \n  + Run in the console \n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install necessary packages for sentiment analysis\ninstall.packages(c(\"tidyverse\", \"tidytext\", \"textdata\"))\n```\n:::\n\n\n+ note: tidyverse for data tasks; tidytext for text analysis; textdata for sentiment dictionaries\n\n::: notes\n\n+ Step 1 - 10 minutes\n\n+ Ensure textdata package is installed, as it provides AFINN, Bing, and NRC lexicons\n\n+ **Content:**  \n  +  Explain sentiment analysis: \"It’s like teaching a computer to read emotions in text, like excitement or fear about AI.\"\n  + Link to AI and Society: \"We’ll analyze comments about AI to see how people feel about its impact on jobs or schools.\"  \n  + RStudio basics: Console (for commands), script editor (for saving code)\n\n+ Practice (3 minutes):  \n\n+ Type print(\"Hello, I’m learning R!\") in the console and press Enter. What happens?  \n  + Guess: What sentiment score might \"excited\" have in a dictionary? \"Sad\"?\n\n+ Discussion (2 minutes):  \n  + Why study sentiments about AI?  \n  + Example: How could knowing if people love or fear AI in schools help teachers?\n\n:::\n\n## Step 2: Loading Tools and Data \n\n+ **Objective: Load R packages and a dataset of comments**\n\n+ Dataset\n  + Fictional social media comments about AI’s societal impact\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\n\ncomments <- tibble(\n  id = 1:30,\n  text = c(\n    \"AI is amazing and will make education so much better!\",\n    \"I’m worried AI will take over jobs and leave people unemployed.\",\n    \"AI helps doctors save lives, it’s a game-changer.\",\n    \"I don’t trust AI, it feels creepy and invasive.\",\n    \"AI is okay, but it needs regulation to be safe.\",\n    \"AI in schools is cool, but it’s not perfect.\",\n    \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\",  # Sarcasm\n    \"AI makes healthcare faster and more accurate, love it!\",\n    \"Why does AI know so much about me? It’s unsettling.\",\n    \"AI chatbots are fun to talk to, but sometimes useless.\",\n    \"AI in movies is awesome, makes everything so realistic!\",\n    \"I’m scared AI will control everything one day.\",\n    \"AI helps me study better, it’s like a personal tutor.\",\n    \"AI is overhyped, it’s not as smart as people think.\",  # Mixed\n    \"Using AI for art is creative and inspiring!\",\n    \"AI in cars? No way, I don’t trust self-driving tech.\",\n    \"AI makes my phone so smart, it’s incredible!\",\n    \"I feel like AI is watching me all the time, creepy.\",\n    \"AI in gaming makes battles so epic, I’m hooked!\",\n    \"AI might replace teachers, and that’s not cool.\",\n    \"AI saves time at work, but I miss human interaction.\",\n    \"AI’s fine, but it makes mistakes sometimes.\",  # Neutral\n    \"AI in music creation is a total game-changer!\",\n    \"I’m skeptical about AI making fair decisions.\",\n    \"AI is great, but only if it’s used ethically.\",  # Mixed\n    \"AI makes life easier, but it’s a bit scary too.\",  # Mixed\n    \"AI in agriculture boosts crops, amazing stuff!\",\n    \"I don’t get why everyone loves AI so much.\",  # Negative\n    \"AI tutors are helpful, but they don’t replace real teachers.\",\n    \"AI sounds cool, but I’m not sure it’s safe.\"  # Mixed\n  )\n)\n```\n:::\n\n\n+ **Task 2.1: Run the Code** \n  + Run the code (highlight and press Ctrl+Enter)  \n\n+ **Task 2.2: View the Data**  \n  + `print(comments)`\n  \n+ **Task 2.3: View comments**  \n  + Run `view(comments)` in the console\n    + How many comments are there?  \n\n::: notes\n\n+ 10 minutes\n\n+ **Content:**  \n  + Introduce dataset: \"We’ll analyze comments about AI’s impact on society, like education and jobs\"\n\n+ The dataset now includes 30 varied comments, covering:\n  + Positive: e.g., \"AI makes healthcare faster and more accurate, love it!\"\n  + Negative: e.g., \"I’m scared AI will control everything one day.\"\n  + Neutral: e.g., \"AI’s fine, but it makes mistakes sometimes.\"\n  + Complex: e.g., \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\" (sarcasm), \"AI is great, but only if it’s used ethically\" (mixed emotions).\n  + Complex comments challenge lexicon-based approaches\n    + sarcasm (e.g. comment 7)\n    + mixed sentiments (e.g. comments 14, 25, 26, 30)\n      + AFINN may misinterpret due to its word-based scoring\n\n+ Comments like \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\" (sarcasm) may score positively due to words like \"great\" but are negative in context\n\n+ Mixed comments (e.g., \"AI makes life easier, but it’s a bit scary too\") combine positive and negative words, potentially leading to misleading scores\n\n+ Run `colnames(comments)` and `nrow(comments)`\n\n+ Write down: Which comment seems happiest about AI? Most negative? \n  + Example: \"AI is amazing\" vs. \"AI feels creepy\"\n\n+ Discussion (2 minutes):  \n  + How might these comments reflect real opinions about AI?  \n  + Example: What might someone post about AI in movies or games? \n\n:::\n\n\n## Step 3: Exploring the Dataset\n\n+ **Objective: Understand the dataset’s structure**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(comments)\nnrow(comments)\n\ncomments$text[1]\n```\n:::\n\n\n+ **Task 3.1: View Specific Comments**  \n  + Run `ncol(comments)` to check how many columns are in the dataset\n  \n+ **Task 3.2: View Specific Comments**  \n  + Run `comments$text[4]` to see the fourth comment  \n  \n\n::: notes\n\n+ 5 min\n\n+ Content:  \n  + Dataset has id (comment number) and text (comment content)\n  + Exploring data helps us know what we’re analyzing\n\n+ **Task: View a Specific Comment**\n  + Run `comments$text[3]`\n    + What does comment 3 say?\n \n+ Practice (3 minutes):  \n  + Run comments$text[4]. What does it say?  \n  + Run ncol(comments). How many columns in the dataset?  \n  + Write down: What does id tell us about each comment?\n\n+ Discussion (2 minutes):  \n  + Why explore data before analyzing it?   \n  + How can comments about AI help us understand its societal role?\n\n:::\n  \n  \n## Step 4: Splitting Text into Words\n\n+ **Objective: Learn tokenization to break text into words**\n  \n+ Tokenization splits sentences into words\n  + e.g. \"AI is cool\" $\\rightarrow$ {\"AI,\" \"is,\" \"cool\"}\n  + Words are the building blocks for (most) sentiment analysis\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords <- comments %>%\n  unnest_tokens(word, text)\n```\n:::\n\n\n\n+ **Task 4.1: View Words**  \n  + `print(words)`\n\n+ **Task 4.2: How many words are there?**\n  + Run `nrow(words)`  \n\n+ **Task 4.3: View First 5 Words**  \n  + Run `head(words, 5)`\n  \n+ **Task 4.4: How many unique words are there?**  \n  + Run `n_distinct(words$word)`\n\n+ **Task 4.5: View Most Common Words**  \n  + Run `words %>% count(word, sort = TRUE) %>% head(10)`\n\n+ **Task 4.6: How many times does \"better\" appear?**  \n  + Run `words %>% filter(word == \"better\")`\n\n\n::: notes\n\n+ 12 minutes\n\n+ Guess: Which words might show strong emotions?\n  + Discussion  \n    + Why split text into words?  \n    + How might words like \"amazing\" or \"creepy\" reflect feelings about AI?\n\n+ words %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE) \n\n:::\n\n\n## Step 5: Exploring Sentiment Lexicons\n\n+ **Objective: Understand how lexicons assign sentiment scores**\n\n+ A lexicon is a dictionary scoring words’ emotions\n  + **AFINN**: -5 to +5 \n    + e.g. \"Happy\" = +3, \"scary\" = -2\n  + Alternatives\n    + **Bing**: \n      + a binary classification + positive/negative\n    + **NRC**: \n      + emotion-based (e.g., joy, anger) and positive/negative classifications\n \n    \n+ AI uses lexicons to quantify feelings in text\n\n::: notes\n\n+ 12 minutes\n \n:::\n\n## Step 5: Exploring Sentiment Lexicons (continued)\n\n+ We will use the **AFINN** lexicon, which assigns scores to words based on their sentiment\n  + Positive words have positive scores, negative words have negative scores\n  + Neutral words have a score of 0\n  \n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- get_sentiments(\"afinn\")\n```\n:::\n\n\n\n+ **Task 5.1: View Lexicon**\n  + Run: `head(afinn, 10)`\n\n+ **Task 5.2: Check Scores for Specific Words**\n  + Run: `afinn %>% filter(word == \"trust\")`\n  + What’s its score?  \n  + Run: `afinn %>% filter(word == \"bad\")`\n  + Guess the score for \"awesome\"\n  + List two words you think are negative  \n\n::: notes\n\n+ Check with `afinn %>% filter(word == \"awesome\")`  \n\n+ Check their scores with `afinn %>% filter(word %in% c(\"word1\", \"word2\"))`\n\n+ Discussion (2 minutes):  \n  + How does a lexicon help AI understand text?  \n  + Why might it miss complex emotions (e.g., \"AI is cool but scary\")?\n\n+ get_sentiments(\"bing\") or get_sentiments(\"nrc\")\n\n:::\n\n\n## Step 6: Scoring Words for Sentiment\n\n+ **Objective: Assign sentiment scores to words**\n\n+ Match dataset words to AFINN lexicon scores\n  + Only words in the lexicon get scores\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentiment_scores <- words %>%\n  inner_join(afinn, by = \"word\")\n```\n:::\n\n\n+ **Task 6.1: View Scores**  \n  + Run: `print(sentiment_scores)`\n  + List one positive and one negative word \n  \n+ **Task 6.2: Count Negative Words**  \n  + Run: `sentiment_scores %>% filter(value < 0)`\n\n+ **Task 6.3: Count Positive Words**  \n  + Run: `sentiment_scores %>% filter(value > 0)`\n\n::: notes\n\n+ Guess: Why might some words (e.g., \"AI\") not appear?\n\n+ Discussion (2 minutes):  \n\n+ How do scores show opinions about AI?  \n  + Example: How might negative scores about AI jobs affect workers?\n\n:::\n\n## Step 7: Summarizing Comment Sentiment\n\n+ **Objective: Calculate total sentiment for each comment**\n\n+ Sum word scores per comment to get its overall sentiment\n  + Positive total = positive comment\n  + Negative total = negative comment\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomment_sentiment <- sentiment_scores %>%\n  group_by(id) %>%\n  summarize(total_score = sum(value)) %>%\n  right_join(comments, by = \"id\") %>% \n  arrange(id)\n```\n:::\n\n\n\n+ **Task 7.1: View Results**\n  + Run `print(comment_sentiment)`\n  + Which comment has the highest total_score? Lowest?  \n\n+ **Task 7.2: Sort by Total Score**\n  + Run `comment_sentiment %>% arrange(desc(total_score))`\n  + Which is most positive?  \n\n+ **Task 7.3: Check Comment 18’s Score**\n  + Read comment 18’s text and score\n  + Do they match?  \n\n+ **Task 7.4: Check Neutral Comments**\n  + Run `comment_sentiment %>% filter(total_score == 0)`\n  + Any neutral comments?\n\n+ **Task 7.5: Add Sentiment Labels**\n  + Run the following code\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomment_sentiment <- comment_sentiment %>% \n  mutate(sentiment = case_when(is.na(total_score) ~ NA_character_,\n                               total_score > 0 ~ \"Positive\", \n                               total_score < 0 ~ \"Negative\",\n                               TRUE ~ \"Neutral\"))\n```\n:::\n\n\n::: notes\n\n+ Step 7 - 15 minutes\n\n+ Discussion (2 minutes):  \n  + What do scores tell us about views on AI?  \n  + Example: How might positive scores about AI in healthcare help doctors?\n\n+ **Task 7.5: Add a New Comment**  \n  + Add a new comment\n    `comments <- comments %>% add_row(id = 31, text = \"AI makes learning fun but can be confusing.\")`\n    + What’s the new comment’s score?  \n    + Run comment_sentiment %>% filter(total_score > 0) -  How many positive comments?\n  \n:::\n\n\n## Step 8: Visualizing and Discussing Results\n\n+ **Objective: Visualize sentiment and discuss AI’s societal impact**\n\n+ Create a bar plot to see positive/negative sentiments\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comment_sentiment, aes(x = id, y = total_score, fill = sentiment)) +\n  geom_bar(stat = \"identity\", na.rm = TRUE) +\n  geom_text(\n    data = filter(comment_sentiment, !is.na(total_score)),\n    aes(\n      label = total_score,\n      vjust = case_when(\n        total_score >= 0 ~ -0.3,\n        total_score < 0  ~ 1.3\n      )\n    )\n  ) +\n  geom_text(\n    data = filter(comment_sentiment, is.na(total_score)),\n    aes(y = 0, label = \"NA\"),\n    vjust = -0.3,\n    color = \"black\",\n    size = 2\n  ) +\n  labs(title = \"Sentiment Scores of Comments about AI usign AFINN\",\n       x = \"Comment ID\", y = \"Sentiment Score\") +\n  scale_fill_manual(\n    name = \"Sentiment\",\n    values = c(\"Negative\" = \"red\", \"Positive\" = \"blue\"),\n    labels = c(\"Negative\", \"Positive\"),\n    na.translate = FALSE # don't show NA in the legend\n  ) +\n  scale_x_continuous(breaks = seq(2, 30, by = 2)) +\n  theme_minimal()\n```\n:::\n\n\n\n+ **Task 8.1: Create Plot** \n  + Run the plot code  \n  + Identify: Which comments are blue (positive)? Red (negative)?\n\n+ **Task 8.2: Plot the density of sentiment distribution**\n  + Create a density plot of sentiment scores\n  + Use `geom_density()` to visualize the distribution\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comment_sentiment, aes(x = total_score, fill = sentiment)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Comment Sentiment Scores\",\n       x = \"Sentiment Score\", y = \"Density\") +\n  theme_minimal()\n```\n:::\n\n\n+ **Task 8.3: Plot the histogram of sentiment label**  \n  + Create a histogram of sentiment labels\n  + Use `geom_bar()` to visualize counts of each sentiment label\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comment_sentiment, aes(x = sentiment, fill = sentiment)) +\n  geom_bar() +\n  geom_text(\n    stat = \"count\",\n    aes(label = after_stat(count)),\n    vjust = -0.5\n  ) +\n  labs(title = \"Histogram of Sentiment Labels\",\n       x = \"Sentiment\", y = \"Count\") +\n  theme_minimal()\n```\n:::\n\n\n\n::: notes\n\n+ Step 8 - 24 minutes\n\n+ Practice (6 minutes)\n  + Run comment_sentiment %>% filter(id == 7). Why might the sarcastic comment’s score be misleading?\n  + Try the Bing lexicon: Replace afinn with get_sentiments(\"bing\") in Step 6 and rerun Steps 6–8\n    + Compare results\n    \n:::\n\n## Step 9: Exploring other Lexicon\n\n**Objective:** Explore Bing and NRC sentiment lexicons as alternatives to AFINN\n\n+ What are Bing and NRC? (recap)\n  + **Bing Lexicon:**  \n    + Classifies words as \"positive\" or \"negative\" only (no score)\n  + **NRC Lexicon:**  \n    + Assigns words to emotions (joy, anger, fear, etc.) and positive/negative\n\n+ **Tasks: Load Bing and NRC Lexicons**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n\nbing <- get_sentiments(\"bing\")\n\nnrc <- get_sentiments(\"nrc\")\n```\n:::\n\n\n+ **Task: View Lexicon Examples**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(bing, 10)\n\nhead(nrc, 10)\n```\n:::\n\n\n+ **Task: Join Words with Bing**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_bing <- words %>%\n  inner_join(bing, by = \"word\")\n\nwords_nrc <- words %>%\n  inner_join(nrc, by = \"word\")\n```\n:::\n\n\n::: notes\n\n+ some words show up more than 1 time\n\nword_counts <- bing %>%\n  group_by(word) %>%\n  summarise(count = n()) %>%\n  filter(count > 1)\n  \n:::\n\n\n\n## Step 9: Exploring other Lexicon (continued)\n\n+ **Summarize Sentiment by Comment (Bing)**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomment_sentiment_bing <- words_bing %>%\n  group_by(id, sentiment) %>%\n  summarise(word_count = n(), .groups = \"drop\") %>%\n  pivot_wider(names_from = sentiment, values_from = word_count, values_fill = 0) %>%\n  right_join(comments, by = \"id\") %>% \n  mutate(total_score = positive - negative) %>%\n  mutate(sentiment = case_when(\n      is.na(total_score) ~ NA_character_,\n      total_score > 0 ~ \"Positive\",\n      total_score < 0 ~ \"Negative\",\n      TRUE ~ \"Neutral\"\n    )) %>% arrange(id)\n```\n:::\n\n\n. . .\n\n+ **Summarize Sentiment by Comment (NRC)**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_nrc_pn <- words_nrc %>% filter(sentiment %in% c(\"positive\", \"negative\"))\n\ncomment_sentiment_nrc <- words_nrc_pn %>%\n  group_by(id, sentiment) %>%\n  summarise(word_count = n(), .groups = \"drop\") %>%\n  pivot_wider(names_from = sentiment, values_from = word_count, values_fill = 0) %>%\n  right_join(comments, by = \"id\") %>%\n  mutate(total_score = positive - negative) %>%\n  mutate(sentiment = case_when(\n    is.na(total_score) ~ NA_character_,\n    total_score > 0 ~ \"Positive\",\n    total_score < 0 ~ \"Negative\",\n    TRUE ~ \"Neutral\"\n  )) %>% arrange(id)\n```\n:::\n\n\n+ **Task 9.1: Compare Positive and Negative Comments**  \n  + Run `comment_sentiment_bing %>% filter(positive > 0)`  \n  + Run `comment_sentiment_nrc %>% filter(positive > 0)`  \n  + Which comments are positive by Bing? Which by NRC?\n+ **Task 9.2: Visualize Bing Results**\n  + Create bar plots to visualize positive and negative word counts per comment\n  + Use `geom_bar()` to show counts of positive and negative words\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(comment_sentiment_bing, aes(x = sentiment)) +\n  geom_bar(fill = \"blue\", alpha = 0.5) +\n  labs(title = \"Bing Lexicon: Histogram of Sentiment\", x = \"Sentiment\", y = \"Count\") +\n  geom_text(stat = \"count\", aes(label = after_stat(count)), vjust = -0.5) +\n  theme_minimal()\n```\n:::\n\n\n+ **Task 9.3 compare Bing with AFINN**\n  + Compare Bing and AFINN results\n  + Create a comparison dataframe with both lexicons\n  + Use `left_join()` to merge AFINN and Bing results by comment ID\n  + Identify comments where Bing and AFINN disagree\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison_df2 <- comments %>% \n  left_join(comment_sentiment_bing %>% select(id, sentiment), by = \"id\") %>%\n  rename(sentiment_bing = sentiment) %>%\n  left_join(comment_sentiment %>% select(id, sentiment), by = \"id\") %>%\n  rename(sentiment_afinn = sentiment)\n\ncomparison_df2\n\n# show the comments where Bing and AFINN disagree\n\ncomparison_df2 %>%\n  filter(sentiment_bing != sentiment_afinn | is.na(sentiment_bing) != is.na(sentiment_afinn))\n```\n:::\n\n\n\n::: notes\n\n+ Bing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comment_sentiment_bing, \n       aes(x = id, y = total_score, fill = total_score > 0)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = total_score),\n    vjust = -0.3\n  ) +\n  labs(title = \"Sentiment Scores of Comments about AI using Bing lexicon\",\n       x = \"Comment ID\", y = \"Sentiment Score\") +\n  scale_fill_manual(\n    name = \"Sentiment\",\n    values = c(\"red\", \"blue\"), \n    labels = c(\"Negative\", \"Positive\"),\n    na.translate = FALSE\n  ) +\n  scale_x_continuous(breaks = seq(2, 30, by = 2)) +\n  theme_minimal()\n```\n:::\n\n\n\n+ **Task 9.3: Visualize NRC Emotions**  \n  + Try visualizing NRC emotions (joy, anger, etc.) using a bar plot\n    + Use `geom_bar()` to show counts of each emotion per comment\n    + Example code:\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_nrc_emotions <- words_nrc %>%\n    filter(!sentiment %in% c(\"positive\", \"negative\")) %>%\n    group_by(id, sentiment) %>%\n    summarise(word_count = n(), .groups = \"drop\")\n\nggplot(words_nrc_emotions, aes(x = sentiment, y = word_count, fill = sentiment)) +\n    geom_bar(stat = \"identity\") +\n    facet_wrap(~id, ncol = 6) +\n    labs(title = \"NRC Emotions per Comment\", x = \"Emotion\", y = \"Word Count\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n:::\n\n\n+ **Task 9.4: Discuss Lexicon Differences**\n  + Which lexicon (AFINN, Bing, NRC) seems best for your dataset? \n    + Consider factors like vocabulary coverage, sentiment granularity, and performance on your specific comments.\n+ **Task 9.5: Discuss Nuance and Sarcasm**\n    + Does NRC capture more nuance?  \n        + How do results compare for sarcasm or mixed comments?\n        + Discuss how NRC’s emotion-based approach might better capture complex sentiments like sarcasm or mixed emotions compared to AFINN or Bing\n+ **Task 9.6: Discuss Lexicon Limitations**\n    + Discuss how lexicons like AFINN and Bing may struggle with sarcasm or mixed sentiments\n        + e.g., \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\"  \n        + AFINN might score \"great\" positively, missing the sarcasm\n\n:::\n\n## Step 10 Sentiment Analysis with Ollama\n\n+ **Objective: Use Ollama with Llama 3.2:3b to perform sentiment analysis**\n\n+ Ollama runs large language models (LLMs) like Llama 3.2:3b locally\n  + offering nuanced sentiment analysis by understanding context\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ollamar\")\n```\n:::\n\n\n+ Load Ollama \n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ollamar)\n\ntest_connection()\n\nlist_models()\n\n#pull(\"llama3.1\")  # download a model (equivalent bash code: ollama run llama3.1)\n```\n:::\n\n\n+ testing\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate a response/text based on a prompt; returns an httr2 response by default\nresp <- generate(model=\"llama3.2:3b\", prompt=\"tell me a 50-word story\")\nresp\n\n# get just the text from the response object\nresp_process(resp, \"text\")\n\n# get the text as a tibble dataframe\nresp_process(resp, \"df\")\n```\n:::\n\n\n\n## Step 10 Sentiment Analysis with Ollama (continued)\n\n+ Define the function to get sentiment using Ollama\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiment_ollama <- function(text) {\n  prompt <- paste(\"Classify the sentiment of the following text as Positive, Negative, or Neutral, and respond with only the label:\", text)\n  response <- generate(model = \"llama3.2:3b\", prompt = prompt, output=\"text\")\n  return(response)\n}\n```\n:::\n\n\n+ **Task 9.1: Test the Function**  \n  + Run `get_sentiment_ollama(\"AI is amazing and will make education so much better!\")` \n  + What sentiment does it return?\n  \n+ **Task 9.2: Analyze All Comments**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ncomments_ollama <- comments %>% mutate(sentiment_ollama = map_chr(text, get_sentiment_ollama))\n```\n:::\n\n\n\n## Step 11: Compare with lexicon-based approach\n\n+ Compare with  lexicon to see differences\n  + especially in complex comments (e.g. sarcasm, mixed emotions)\n\n+ Add to comment_sentiment and categorize lexicon sentiments\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomment_sentiment3 <- comparison_df2 %>% \n  left_join(comments_ollama %>% select(id, sentiment_ollama), by = \"id\")\n```\n:::\n\n\n\n+ **Task 11.1: Visualize Sentiment Comparison**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\n\ncomparison3 <- comment_sentiment3 %>%\n  select(id, sentiment_afinn, sentiment_bing, sentiment_ollama) %>%\n  pivot_longer(cols = c(sentiment_afinn, sentiment_bing, sentiment_ollama), \n               names_to = \"method\", \n               values_to = \"sentiment\")\n\ncomparison_counts3 <- comparison3 %>%\n  count(method, sentiment)\n\nggplot(comparison_counts3, aes(x = sentiment, y = n, fill = method)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Sentiment Distribution Comparison\", \n       x = \"Sentiment\", y = \"Count\") +\n  scale_fill_manual(name = \"Method\",\n                    values = c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"), \n                    labels = c(\"AFINN (Lexicon)\", \"Bing (Lexicon)\", \"llama3.2:3b (LLM)\")) +\n  scale_y_continuous(breaks = seq(0, max(comparison_counts3$n, na.rm = TRUE) + 3, by = 3)) +\n  theme_minimal()\n```\n:::\n\n\n::: notes\n\n+ geom_text(aes(label = n), position = position_dodge(width = 0.45), hjust= 1., vjust = -0.5) +\n\nlibrary(ggplot2)\n\nggplot(comparison_counts3, aes(x = sentiment, y = n, fill = method)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\", linewidth = 0.2) +\n  geom_text(aes(label = n), position = position_dodge(width = 0.45), vjust = -0.5, \n            size = 3.5, fontface = \"bold\") +\n  labs(title = \"Comparison of Sentiment Distribution Across Methods\", \n       subtitle = \"Counts of Sentiment Categories by Analysis Method\",\n       x = \"Sentiment Category\", \n       y = \"Number of Comments\",\n       caption = \"Data source: Sentiment analysis of comments using AFINN, Bing, and Ollama\") +\n  scale_fill_manual(name = \"Sentiment Analysis Method\", \n                    values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\"), \n                    labels = c(\"Lexicon (AFINN)\", \"Lexicon (Bing)\", \"Ollama (LLM)\")) +\n  scale_y_continuous(breaks = seq(0, max(comparison_counts3$n, na.rm = TRUE) + 3, by = 3),\n                     expand = expansion(mult = c(0, 0.1))) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 10, hjust = 0, color = \"grey50\"),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(color = \"black\"),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(size = 10),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(10, 10, 10, 10)\n  )\n:::\n\n## Step 11: compare with lexicon-based approach (continued)\n\n+ **Task 11.2: Identify Differing Comments**\n  + Find comments where `sentiment_lexicon != sentiment_ollama`\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiffering_comments <- comment_sentiment3 %>%\n  filter(sentiment_afinn != sentiment_ollama | sentiment_bing !=sentiment_ollama | sentiment_afinn != sentiment_ollama) %>%\n  select(id, text, sentiment_afinn, sentiment_bing, sentiment_ollama)\n\nprint(differing_comments)\n```\n:::\n\n\n+ **Task 11.3: Examine Specific Comments**\n  + Run `comment_sentiment %>% filter(id %in% c(7, 14, 26))`\n  + Why might the lexicon and LLM differ for these comments?  \n    + Discuss how LLMs capture context (e.g., sarcasm) better than lexicons\n\n\n::: notes\n\n+ ID 7: \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\" (sarcasm)\n\n+ ID 14: \"AI is overhyped, it’s not as smart as people think.\" (mixed)\n\n+ ID 26: \"AI makes life easier, but it’s a bit scary too.\" (mixed)\n\n+ Discussion\n  + Why do comments like ID 7 (sarcasm) differ? \n    + Lexicon may score \"great\" positively\n    + but Llama 3.2:3b detects sarcasm.\n\n+ Comments with Significant Differences\n  + The following comments are likely to show discrepancies between AFINN and Llama 3.2:3b, as highlighted in Task 9.3:\n  + ID 7: \"Wow, AI is so great, it’ll solve all our problems… yeah, right!\"\n    + AFINN: Positive (due to \"great\" +3, \"solve\" +2), ignoring sarcastic \"yeah, right.\"\n    + Llama 3.2:3b: Negative, detecting sarcasm through context.\n      + Why Different: Lexicons score words individually, missing tone; LLMs analyze full sentences.\n  + ID 14: \"AI is overhyped, it’s not as smart as people think.\"\n    + AFINN: Neutral or negative (e.g., \"overhyped\" -2, \"smart\" +2 may cancel out).\n    + Llama 3.2:3b: Negative, recognizing critical intent.\n    + Why Different: AFINN averages word scores; Llama 3.2:3b captures overall sentiment.\n  + ID 26: \"AI makes life easier, but it’s a bit scary too.\"\n    + AFINN: Neutral (e.g., \"easier\" +2, \"scary\" -2 balance out).\n    + Llama 3.2:3b: Negative or mixed, emphasizing \"scary\" in context.\n  + Why Different: Lexicons struggle with mixed emotions; LLMs weigh context.\n\n+ These differences spark discussion in Step 9 about lexicon limitations (e.g., missing sarcasm) and LLM strengths (e.g., contextual understanding\n\n+ How do LLMs improve on lexicons for complex sentiments?\n  + What are the trade-offs (e.g., LLM complexity vs. lexicon simplicity)?\n\n+ Note for Facilitators: Ensure Ollama is installed and Llama 3.2:3b is pulled (ollama pull llama3.2:3b)\n  + This step may take longer due to LLM processing\n    + consider pre-computing results or running on a subset if time is limited\n\n  + Run comment_sentiment %>% filter(id == 14). Why might the lexicon and LLM differ\n\n+ Try the NRC lexicon: Replace afinn with get_sentiments(\"nrc\") in Step 6\n  + (adjust for emotion categories) and rerun Steps 6–8\n  + Compare with Ollama\n  + Highlight how LLMs capture context (e.g., sarcasm in ID 7) better than lexicons\n\n:::\n\n## More Discussion\n\n+ Share one comment and its score\n  + Does it match the text’s tone?  \n\n+ How does sentiment analysis help understand AI’s societal impact? \n\n+ Companies: Improve AI based on feedback  \n  + e.g. if many comments are negative about AI in jobs, they can address concerns\n  \n+ Governments\n  + Address fears about AI privacy or jobs\n\n+ Society: Highlight excitement for AI in education or healthcare\n\n+ **Wrap-up Questions:**\n  + What surprised you about the comments?\n  + How might sentiment analysis help shape AI’s future in society?\n  + What are the limits of sentiment analysis?  \n\n::: notes\n\n+ e.g. it might miss sarcasm or cultural nuances\n\n:::\n\n\n## Takeaway  \n\n+ Sentiment analysis is an AI tool to understand emotions in text\n\n+ You’ve learned R to \n  + load data\n  + tokenize\n  + score and classify sentiments\n  + visualize sentiments  \n\n\n## Resources {visibility=\"uncounted\"}  \n\n+ [R for Data Science](https://r4ds.had.co.nz/)  \n\n+ [Tidytext Book](https://www.tidytextmining.com/)  \n\n+ [Kaggle](https://www.kaggle.com/) - Search \"Twitter sentiment\" for practice datasets\n\n::: notes\n\n+ Facilitator Notes\n  + Preparation\n    + Ensure R and RStudio are installed. Pre-install packages (tidyverse, tidytext, textdata) to save time \n  + Dataset\n    + Small, fictional dataset keeps it simple. For advanced students, consider a real dataset (e.g., tweets) in a follow-up  \n  + Pacing\n    + Each step is 10–12 minutes, with 24 minutes for Step 8. Skip one practice task per step if running over\n    + Total: ~95 minutes hands-on + ~15 minutes discussion = ~110 minutes  \n  + Engagement\n    + Encourage sharing in discussions to connect to real-world AI issues\n  + Troubleshooting: Assist with RStudio (console vs. script) and package errors\n\n:::\n\n## **Assignment: Real-World Sentiment Analysis Practice**\n\nAnalyze Social Media Data with Lexicon and LLM Methods\n\n+ **Objective:**  Practice everything learned using a real-world dataset \n\n+ **Step 1: Download and Load Data**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nreal_comments <- read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/refs/heads/master/twitter_sentiment.csv\", col_names = c(\"id\",\"entity\", \"sentiment\", \"text\"))\n\nhead(real_comments)\n```\n:::\n\n\n+ **Step 2: Tokenize and Clean Data**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\nreal_words <- real_comments %>%\n  unnest_tokens(word, text)\n```\n:::\n\n\n+ **Step 3: Compare Lexicons among AFINN, Bing and NRC**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- get_sentiments(\"afinn\")\nbing <- get_sentiments(\"bing\")\nnrc <- get_sentiments(\"nrc\")\n\n# Join and score with AFINN\nreal_sentiment_afinn <- real_words %>%\n  inner_join(afinn, by = \"word\") %>%\n  group_by(id) %>%\n  summarize(total_score = sum(value, na.rm = TRUE))\n\n# Join and score with Bing\nreal_sentiment_bing <- real_words %>%\n  inner_join(bing, by = \"word\") %>%\n  group_by(id, sentiment.y) %>%\n  summarize(word_count = n(), .groups = \"drop\") %>%\n  pivot_wider(names_from = sentiment.y, values_from = word_count, values_fill = 0)\n\n# Join and score with NRC (positive/negative)\nreal_sentiment_nrc <- real_words %>%\n  inner_join(nrc %>% filter(sentiment %in% c(\"positive\", \"negative\")), by = \"word\") %>%\n  group_by(id, sentiment.y) %>%\n  summarize(word_count = n(), .groups = \"drop\") %>%\n  pivot_wider(names_from = sentiment.y, values_from = word_count, values_fill = 0)\n```\n:::\n\n\n+ **Step 4: Visualize Results**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AFINN\nlibrary(ggplot2)\nggplot(real_sentiment_afinn, aes(x = id, y = total_score)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"AFINN Sentiment Scores\", x = \"Comment ID\", y = \"Score\") +\n  theme_minimal()\n\n# Bing\nggplot(real_sentiment_bing, aes(x = id)) +\n  geom_bar(aes(y = positive), stat = \"identity\", fill = \"blue\", alpha = 0.5) +\n  geom_bar(aes(y = -negative), stat = \"identity\", fill = \"red\", alpha = 0.5) +\n  labs(title = \"Bing Lexicon: Positive vs Negative\", x = \"Comment ID\", y = \"Word Count\") +\n  theme_minimal()\n```\n:::\n\n\n+ **Step 5: (Optional) Use Ollama/LLM for Sentiment**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you have Ollama and Llama3 installed:\nlibrary(ollamar)\nget_sentiment_ollama <- function(text) {\n  prompt <- paste(\"Classify the sentiment of the following text as positive, negative, or neutral, and respond with only the label in lower case:\", text)\n  response <- generate(model = \"llama3.2:3b\", prompt = prompt, output=\"text\")\n  return(response)\n}\nreal_comments <- real_comments %>%\n  mutate(sentiment_ollama = map_chr(text, get_sentiment_ollama))\n```\n:::\n\n\n. . .\n\n+ **Step 6: Compare and Discuss**\n\n- Compare lexicon and LLM results\n- Which method best handles sarcasm, mixed emotions, or context?\n- Write a short paragraph (3–5 sentences) on your findings\n\n\n::: notes\n\n+ **Deliverables:**  \n  - R script with code for all steps  \n  - Plots comparing sentiment by method  \n  - Short discussion of your results\n\n+ **Bonus:**  \n    + Try analyzing sentiment on comments about another societal topic (e.g. climate change, education, public health)\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}